{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68585f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def C_step(T):\n",
    "  b = np.zeros_like(T)\n",
    "  b[np.arange(len(T)), T.argmax(1)] = 1\n",
    "  T = b\n",
    "\n",
    "  return T\n",
    "\n",
    "def smooth(data, method, coeff):\n",
    "  \n",
    "  if method == \"Laplace\":\n",
    "    return (data + coeff)/(1 + coeff)\n",
    "  elif method == \"None\":\n",
    "    return data\n",
    "  else:\n",
    "    print(\"Unknown Method \" + method + \", returning unchanged data by default.\")\n",
    "    return data\n",
    "\n",
    "def dawid_skene(N, max_iter = 1000, collapse = C_step, check_convergence = False, tol = 0.001, smoothing = \"Laplace\", C = False):\n",
    "\n",
    "  N = N.astype(np.float64)\n",
    "  I, J, K = np.shape(N)\n",
    "  \n",
    "  ## Majority Vote initialization\n",
    "  K_sum = np.sum(N, axis = 2)\n",
    "  T = K_sum/(np.sum(K_sum, axis = 1).reshape(-1, 1))\n",
    "  \n",
    "  ## Collapses the probabilities to a single estimated label per task\n",
    "  if C:\n",
    "    T = collapse(T)\n",
    "\n",
    "  ## EM Algorithm\n",
    "\n",
    "  for i in range(max_iter):\n",
    "\n",
    "    ## Maximization Step:\n",
    "\n",
    "    ## Computes Prior Probability of each class\n",
    "    p = np.mean(T, axis = 0).reshape(-1, 1)\n",
    "\n",
    "    ## Unnormalized Confusion Tensor (J X J X K) \n",
    "    confusion = np.tensordot(T, N, axes = ([0,0]))\n",
    "    \n",
    "    ## Smooths the unnormalized confusion to avoid divide-by-zero when normalizing.\n",
    "    confusion = smooth(confusion, smoothing, 0.01)\n",
    "\n",
    "    ## Normalizes the Confusion Tensor\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.simplefilter(\"error\")\n",
    "      confusion /= np.repeat(np.sum(confusion, axis = 1), J, axis = 0).reshape(J, J, K)\n",
    "    \n",
    "    ## Expectation Step:\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "      warnings.simplefilter(\"ignore\")\n",
    "\n",
    "      ## Compute Log-Likelihood\n",
    "      log_conf = np.log(confusion)\n",
    "      num = np.tensordot(N, log_conf, axes = ((2, 1), (2, 1)))\n",
    "      denom = np.repeat(np.log(np.matmul(np.exp(num), p.reshape(-1, 1))), J, axis = 1)\n",
    "      log_like = np.log(p.reshape(1, -1)) + num - denom\n",
    "\n",
    "    ## Translate Log-Likelihoods to Label Probabilities\n",
    "    T_new = np.exp(log_like).astype('float64')\n",
    " \n",
    "    if C:\n",
    "      T_new = collapse(T_new)\n",
    "\n",
    "    if check_convergence and np.mean(np.abs(T_new - T)) < tol:\n",
    "      T = T_new\n",
    "      break\n",
    "    else:\n",
    "      T = T_new\n",
    "    \n",
    "  ## Return Label Probabilities Tensor and Best-Estimated Labels\n",
    "  return T, np.argmax(T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0048dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
